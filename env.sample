# Environment variables for the LLM WordSearch application
# Copy this file to .env and fill in your actual values

# ==============================
# AI PROVIDER CONFIGURATION
# ==============================

# Community Provider (OpenRouter) - API Key for the community model
# This is used when no custom LLM is configured in the settings
API_KEY=your_openrouter_api_key_here

# Community Provider Model Name (optional)
# Default: openai/gpt-oss-20b:free
# Other popular options: google/gemini-2.5-flash, anthropic/claude-3-haiku, meta-llama/llama-3.1-8b-instruct
COMMUNITY_MODEL_NAME=openai/gpt-oss-20b:free

# Language Model Mapping (optional)
# JSON object to specify different models for different languages
# Example: {"es": {"model": "meta-llama/llama-3.1-8b-instruct"}, "fr": {"model": "anthropic/claude-3-haiku"}}
LANGUAGE_MODEL_MAP={}

# ==============================
# DEVELOPMENT CONFIGURATION
# ==============================

# Development server port (default: 5173)
# VITE_DEV_PORT=5173

# API base URL if using a different backend (optional)
# VITE_API_BASE_URL=https://api.example.com

# ==============================
# BUILD CONFIGURATION
# ==============================

# Build mode settings
# VITE_BUILD_MODE=production

# Enable/disable source maps in production build
# VITE_SOURCEMAP=false

# ==============================
# FEATURE FLAGS
# ==============================

# Enable/disable experimental features
# VITE_ENABLE_EXPERIMENTAL=false

# Enable/disable debug logging
# VITE_DEBUG_LOGGING=false