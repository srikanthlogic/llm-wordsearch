# Environment variables for the LLM WordSearch application
# Copy this file to .env and fill in your actual values

# ==============================
# AI PROVIDER CONFIGURATION
# ==============================

# Community Provider (OpenRouter) - API Key for the community model
# This is used when no custom LLM is configured in the settings
# Get your API key from: https://openrouter.ai/keys
API_KEY=your_openrouter_api_key_here

# Community Provider Model Name (optional)
# Default: google/gemini-2.5-flash
# Popular options:
# - openai/gpt-oss-20b:free (free tier)
# - google/gemini-2.5-flash (recommended)
# - anthropic/claude-3-haiku
# - meta-llama/llama-3.1-8b-instruct
# - mistralai/mistral-7b-instruct
COMMUNITY_MODEL_NAME=google/gemini-2.5-flash

# Language-specific model overrides (optional)
# JSON object to specify different models for different languages
# Example: {"es": {"model": "meta-llama/llama-3.1-8b-instruct"}, "fr": {"model": "anthropic/claude-3-haiku"}}
# Format: {"language_code": {"model": "model-name", "baseURL": "https://api.example.com"}}
LANGUAGE_MODEL_MAP={}

# ==============================
# LLM PROXY CONFIGURATION
# ==============================

# Enable/disable LLM proxy (default: false for local development, true for Vercel)
# Set to "true" to use the proxy, "false" to use direct API calls
# When using Vercel deployment, this should be set to "true"
USE_LLM_PROXY=false

# Custom proxy URL (optional, defaults to /api/llm-proxy)
# This is useful if you're hosting the proxy on a different endpoint
# Examples: https://your-domain.com/api/llm-proxy, /custom-proxy-path
LLM_PROXY_URL=/api/llm-proxy

# ==============================
# DEVELOPMENT CONFIGURATION
# ==============================

# Development server port (default: 5173)
# VITE_DEV_PORT=5173

# API base URL if using a different backend (optional)
# VITE_API_BASE_URL=https://api.example.com

# ==============================
# BUILD CONFIGURATION
# ==============================

# Build mode settings
# VITE_BUILD_MODE=production

# Enable/disable source maps in production build
# VITE_SOURCEMAP=false

# ==============================
# FEATURE FLAGS
# ==============================

# Enable/disable experimental features
# VITE_ENABLE_EXPERIMENTAL=false

# Enable/disable debug logging
# VITE_DEBUG_LOGGING=false

# ==============================
# VERCEL DEPLOYMENT CONFIGURATION
# ==============================

# These variables should be set in the Vercel dashboard for production deployment:
# 
# Required:
# - API_KEY: Your OpenRouter API key
#
# Optional:
# - COMMUNITY_MODEL_NAME: Default model for the community provider
# - LANGUAGE_MODEL_MAP: Language-specific model configurations
# - USE_LLM_PROXY: Set to "true" for Vercel deployment
#
# To set these in Vercel:
# 1. Go to your project dashboard
# 2. Navigate to Settings > Environment Variables
# 3. Add each variable with its value
# 4. Deploy your project

# ==============================
# LOCAL DEVELOPMENT
# ==============================

# For local development, you can use this .env file directly
# Make sure to replace placeholder values with your actual configuration

# Example local development setup:
# API_KEY=sk-or-xxxxx
# COMMUNITY_MODEL_NAME=google/gemini-2.5-flash
# USE_LLM_PROXY=false
# VITE_DEBUG_LOGGING=true